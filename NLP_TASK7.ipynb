{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络一般形式如下图\n",
    "![cnn](8.png)\n",
    "神经网络对于很多分类问题有了很好的结果，但是这是需要一个前提的，这个前提就是分类的类别相对较少。如果当你面临比较多的分类问题的时候，你还想用神经网络，那么就会出现两个问题：\n",
    "1）深的网络结构，更多的神经元，浪费资源\n",
    "2）过拟合\n",
    "针对以上问题，发展出了很多其他的神经网络，比如卷积神经网络\n",
    "\n",
    "# 卷积神经网络\n",
    " \n",
    "## 卷积运算的定义、动机（稀疏权重、参数共享、等变表示）\n",
    "\n",
    "1.卷积的定义：\n",
    "卷积是对两个实变函数的一种数学运算。\n",
    "卷积运算通常用星号表示：s(t)=(x∗w)(t)\n",
    "在卷积网络的术语中，卷积的第一个参数（函数 x）通常叫做输入(input)，第二个参数(函数 w)叫做核函数(kernel function)，输出被称作特征映射(feature map)。\n",
    "\n",
    "在机器学习的应用中，输入通常是多维数组的数据，而核通常是由学习算法优化得到的多维数组的参数。我们通常假设在存储了数值的有限点集以外，卷积函数的值都为零，因而我们可以通过对有限个数组元素的求和来计算卷积。卷积运算通常会在多个维度上进行。\n",
    "\n",
    "2.卷积的动机：\n",
    "卷积运算通过三个重要的思想来帮助改进机器学习系统： 稀疏交互(sparse interactions)、参数共享(parameter sharing)、等变表示(equivariant representations)。另外，卷积提供了一种处理大小可变的输入的方法。\n",
    "\n",
    "1）稀疏交互：传统的神经网络使用矩阵乘法来建立输入与输出的连接关系，每一个输出单元与每一个输入单元都产生交互。然而，卷积网络具有稀疏交互(sparse interactions)的特征，这是通过使核的大小远小于输入的大小来达到的。\n",
    "\n",
    "如果有 m 个输入和 n 个输出，那么矩阵乘法需要 m×n个参数并且相应算法的时间复杂度为 O(m×n)。如果我们限制每一个输出拥有的连接数为 k，那么稀疏的连接方法只需要 k×n 个参数以及O(k×n) 的运行时间。在实际应用中，只需保持 k 比 m小几个数量级，就能在机器学习的任务中取得好的表现。\n",
    "\n",
    "2）参数共享：参数共享(parameter sharing)是指在一个模型的多个函数中使用相同的参数。\n",
    "\n",
    "在传统的神经网络中，当计算一层的输出时，权重矩阵的每一个元素只使用一次。而在卷积神经网络中，核的每一个元素都作用在输入的每一位置上（是否考虑边界像素取决于对边界决策的设计）。卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对于每一位置都需要学习一个单独的参数集合。\n",
    "\n",
    "3）等变表示：如果一个函数满足输入改变，输出也以同样的方式改变这一性质，我们就说它是等变(equivariant)的。特别地，如果函数 f(x) 与 g(x)满足 f(g(x))=g(f(x))， 我们就说 f(x) 对于变换 g 具有等变性。\n",
    " \n",
    "## 一维卷积运算和二维卷积运算\n",
    "\n",
    "https://blog.csdn.net/xh999bai/article/details/89483673\n",
    " \n",
    "## 池化运算的定义、种类（最大池化、平均池化等）、动机\n",
    "\n",
    "池化运算的定义：\n",
    "我们之所以使用卷积后的特征，是因为图像具有“静态型”的属性，也就意味着在一个图像区域的特征极有可能在另一个区域同样适用。所以，当我们描述一个大的图像的时候就可以对不同位置的特征进行聚合统计（例如：可以计算图像一个区域上的某个特定特征的平均值 or 最大值）这种统计方式不仅可以降低纬度，还不容易过拟合。这种聚合统计的操作就称之为池化，或平均池化、最大池化。\n",
    "种类：\n",
    "1）最大池化：max-pooling，即对邻域内特征点取最大。\n",
    "2）平均池化：mean-pooling，即对邻域内特征点只求平均。\n",
    "池化层的作用:\n",
    "1）下采样\n",
    "2）降维、去除冗余信息、对特征进行压缩、简化网络复杂度、减小计算量、减小内存消耗\n",
    "3）实现非线性\n",
    "4）扩大感知野\n",
    "5）实现不变性，（平移不变性、旋转不变性和尺度不变性）\n",
    "池化的优点：\n",
    "1）显著减少参数数量\n",
    "2）池化单元具有平移不变性，pooling可以保持某种不变性（旋转、平移、伸缩等）\n",
    "\n",
    "训练算法\n",
    "同一般机器学习算法,先定义Loss function,衡量与实际结果之间差距\n",
    "找到最小化损失函数的W和b,CNN中的算法是SGD\n",
    "SGD需要计算W和b的偏导\n",
    "P算法就是计算偏导用的\n",
    "BP算法的核心是求导链式法则\n",
    "总结下来就是：\n",
    "BP算法利用链式求导法则,逐级相乘直到求解出dw和db\n",
    "利用SGD/随机梯度下降,迭代和更新W和b\n",
    "\n",
    "优点\n",
    "共享卷积核，对高维数据处理无压力\n",
    "无需手动选取特征,训练好权重,即得特征\n",
    "分类效果好\n",
    "缺点\n",
    "需要调参,需要大样本量,训练最好要GPU\n",
    "物理含义不明确\n",
    "\n",
    "\n",
    "## Text-CNN的原理\n",
    "\n",
    "![cnn](7.png)\n",
    "https://blog.csdn.net/sun_xiao_kai/article/details/94876240\n",
    "\n",
    "1. embedding layer 嵌入层\n",
    "textcnn使用预先训练好的词向量作embedding layer。对于数据集里的所有词，因为每个词都可以表征成一个向量，因此我们可以得到一个嵌入矩阵\\large M, \\large M里的每一行都是词向量。\n",
    "2. Convolution and pooling layer卷积池化层\n",
    "卷积\n",
    "输入一个句子，首先对这个句子进行切词，假设有s个单词。对每个词，跟句嵌入矩阵, 可以得到词向量。假设词向量一共有d维。那么对于这个句子，便可以得到s行d列的矩阵A.\n",
    "把矩阵A看成是一幅图像，使用卷积神经网络去提取特征。由于句子中相邻的单词关联性总是很高的，因此可以使用一维卷积。卷积核的宽度就是词向量的维度，高度是超参数，可以设置。 \n",
    "池化\n",
    "不同尺寸的卷积核得到的特征(feature map)大小也是不一样的，因此我们对每个feature map使用池化函数，使它们的维度相同。最常用的就是1-max pooling，提取出feature map照片那个的最大值。这样每一个卷积核得到特征就是一个值，对所有卷积核使用1-max pooling，再级联起来，可以得到最终的特征向量，这个特征向量再输入softmax layer做分类。这个地方可以使用drop out防止过拟合\n",
    "\n",
    "\n",
    "## 利用Text-CNN模型来进行文本分类\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class TextCNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        # Embedding part can try multichannel as same as origin paper\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "        convs = []\n",
    "        for kernel_size in [3, 4, 5]:\n",
    "            c = Conv1D(128, kernel_size, activation='relu')(embedding)\n",
    "            c = GlobalMaxPooling1D()(c)\n",
    "            convs.append(c)\n",
    "        x = Concatenate()(convs)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Train...\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 629s 25ms/step - loss: 0.3744 - acc: 0.8258 - val_loss: 0.3102 - val_acc: 0.8679\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 705s 28ms/step - loss: 0.1958 - acc: 0.9231 - val_loss: 0.2447 - val_acc: 0.9014\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 614s 25ms/step - loss: 0.1068 - acc: 0.9645 - val_loss: 0.2693 - val_acc: 0.8953\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 624s 25ms/step - loss: 0.0486 - acc: 0.9880 - val_loss: 0.3101 - val_acc: 0.8928\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 667s 27ms/step - loss: 0.0158 - acc: 0.9983 - val_loss: 0.3799 - val_acc: 0.8907\n",
      "Test data accuracy is  0.89072\n"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 100\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = TextCNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "result = model.predict(x_test)\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('Test data accuracy is ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
