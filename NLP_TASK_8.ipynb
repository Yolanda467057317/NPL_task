{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task8 循环和递归神经网络 (2 days)\n",
    "\n",
    "## RNN的结构。\n",
    "\n",
    "循环神经网络（Recurrent Neural Network，RNN）是一类具有短期记忆能力的神经网络。在循环神经网络中，神经元不但可以接受其它神经元的信息，也可以接受自身的信息，形成具有环路的网络结构。和前馈神经网络相比，循环神经网络更加符合生物神经网络的结构。循环神经网络已经被广泛应用在语音识别、语言模型以及自然语言生成等任务上。循环神经网络的参数学习可以通过随时间反向传播算法(BPTT)[Werbos, 1990]来学习。随时间反向传播算法即按照时间的逆 序将错误信息一步步地往前传递。当输入序列比较长时，会存在梯度爆炸和消失问题，也称为长程依赖问题。\n",
    "\n",
    "![cnn](9.png)\n",
    "RNN按时间展开如上图所示，其中x表示输入层、o表示输出层、s表示隐藏层，U、V、W表示权重参数。\n",
    "以t时刻为例，隐藏层st的输入除了当前时刻输入层的输出xt，还包含上一时刻隐藏层的输出状态st-1。RNN中隐藏层可以完成对信息的记忆，理论上RNN每一时刻的隐藏层都可以完成对上一时刻信息的记忆，也就是说在理论上RNN的隐藏层可以对信息无限记忆，处理任意长度的序列数据，但是在实际中会存在梯度消失或者梯度爆炸的问题，因此，在RNN中隐藏层st完成的只是信息的短时记忆。\n",
    "![cnn](10.png)\n",
    "从上面的结果可以看到，RNN中每一时刻的输出ot都是受前面历次输入xt、xt-1、xt-2…影响的，这也就是为什么说RNN可以处理序列数据的原因。\n",
    "\n",
    "RNN参数计算：\n",
    "RNN中隐藏层参数（包括权重参数、偏置项）\n",
    "隐藏层参数 =（h + x）* h + h\n",
    "其中，h是隐藏层输出的状态向量的维度（该维度和隐藏层神经元个数一致），x是输入层的输出向量维度，（h + x）* h 是权重参数，h是隐藏层中偏置项个数。\n",
    "\n",
    "RNN长期依赖\n",
    "RNN的训练过程和全连接神经网络一样，都是采用反向传播算法通过计算梯度来更新参数。但是在RNN训练过程中会存在长期依赖问题，这是由于RNN在训练时会遇到梯度消失或者梯度爆炸，所谓梯度消失和梯度爆炸是指在训练时计算和反向传播时，梯度在每一时刻都是倾向于递增或者递减的，经过一段时间后，当梯度收敛到零（梯度消失）或发散到无穷大（梯度爆炸），此时，参数就不会再更新，也就是说后期的训练不会再对参数的更新产生影响。形象的说，长期依赖问题就是当时间间隔增大时，当前时刻隐藏层会丧失连接到远处信息的能力。\n",
    "\n",
    "\n",
    "## 双向RNN\n",
    "有时候利用RNN在处理语言模型时，只是基于前面的文本信息是不够的，同时还要考虑后面的信息，综合判断才能够做出预测。\n",
    "比如下面这一句话：\n",
    "我的手机坏了，我打算（）一部新手机。\n",
    "要预测（）中出现的词，如果只是基于前面的文本信息，这里可能是修、买、或者心情不好，大哭一场等，但是如果考虑了后面的信息“一部新手机”，那么这里（）出现“买”这个词的概率就最大。\n",
    "![cnn](11.png)\n",
    "双向循环神经网络的结构按时间展开如上，可以看到，在双向循环神经网络隐藏层中多了一个反向传输的隐藏层\n",
    "\n",
    "\n",
    "##  LSTM、GRU\n",
    "\n",
    "### 长短时记忆网络LSTM结构：\n",
    "RNN神经元中只有这一计算，隐藏层的状态对短期的输入非常敏感，存在梯度消失和梯度爆炸的问题。有科学家提出LSTM解决了这个问题，原始的RNN神经元中只有一个隐藏状态，这个状态对短期输入比较敏感，于是LSTM增加了一个单元状态来记忆长期的信息，如下。所以LSTM的输出有两个，当前时刻LSTM输出值ht和当前时刻的单元状态ct，都是向量。\n",
    "\n",
    "###  LSTM的一种成功变体GRU：\n",
    "LSTM有很多变体，GRU是其中很成功的一个。\n",
    "GRU对LSTM做了两大改动：\n",
    "1、将输入门、遗忘门、输出门变为两个门：更新门（Update Gate）zt和重置门（Reset Gate）rt。\n",
    "2、将单元状态与输出合并为一个状态：h。\n",
    "\n",
    "针对梯度消失（LSTM等其他门控RNN）、梯度爆炸（梯度截断）的解决方案。\n",
    "\n",
    "#### 梯度爆炸解决方法：\n",
    "权重衰减：通过给参数增加 ℓ1 或 ℓ2 范数的正则化项来限制参数的取值范 围，从而使得γ ≤ 1\n",
    "梯度截断：当梯度的模大于一定 阈值时，就将它截断成为一个较小的数\n",
    "####  梯度消失解决方法：\n",
    "合理的初始化权重值。初始化权重，使每个神经元尽可能不要取极大或极小值，以躲开梯度消失的区域。\n",
    "使用relu代替sigmoid和tanh作为激活函数。\n",
    "使用其他结构的RNNs，比如长短时记忆网络（LTSM）和Gated Recurrent Unit（GRU），这是最流行的做法。\n",
    "\n",
    "###  Text-RNN的原理\n",
    "如下是引入双向LSTM进行分类；一般流程是1. embeddding layer, 2.Bi-LSTM layer, 3.concat output, 4.FC layer, 5.softmax\n",
    "![cnn](12.png)\n",
    "长短时记忆网络LSTM结构：\n",
    "RNN神经元中只有这一计算，隐藏层的状态对短期的输入非常敏感，存在梯度消失和梯度爆炸的问题。有科学家提出LSTM解决了这个问题，原始的RNN神经元中只有一个隐藏状态，这个状态对短期输入比较敏感，于是LSTM增加了一个单元状态来记忆长期的信息，如下。所以LSTM的输出有两个，当前时刻LSTM输出值ht和当前时刻的单元状态ct，都是向量。\n",
    "\n",
    "###  LSTM的一种成功变体GRU：\n",
    "LSTM有很多变体，GRU是其中很成功的一个。\n",
    "GRU对LSTM做了两大改动：\n",
    "1、将输入门、遗忘门、输出门变为两个门：更新门（Update Gate）zt和重置门（Reset Gate）rt。\n",
    "2、将单元状态与输出合并为一个状态：h。\n",
    "\n",
    "## 针对梯度消失（LSTM等其他门控RNN）、梯度爆炸（梯度截断）的解决方案。\n",
    "\n",
    "#### 梯度爆炸解决方法：\n",
    "权重衰减：通过给参数增加 ℓ1 或 ℓ2 范数的正则化项来限制参数的取值范 围，从而使得γ ≤ 1\n",
    "梯度截断：当梯度的模大于一定 阈值时，就将它截断成为一个较小的数\n",
    "####  梯度消失解决方法：\n",
    "合理的初始化权重值。初始化权重，使每个神经元尽可能不要取极大或极小值，以躲开梯度消失的区域。\n",
    "使用relu代替sigmoid和tanh作为激活函数。\n",
    "使用其他结构的RNNs，比如长短时记忆网络（LTSM）和Gated Recurrent Unit（GRU），这是最流行的做法。\n",
    "\n",
    "##  Text-RNN的原理\n",
    "假设训练集中所有文本/序列的长度统一为n，我们需要对文本进行分词，并使用词嵌入得到每个词固定维度的向量表示。对于每一个输入文本/序列，我们可以在RNN的每一个时间步长上输入文本中一个单词的向量表示，计算当前时间步长上的隐藏状态，然后用于当前时间步骤的输出以及传递给下一个时间步长并和下一个单词的词向量一起作为RNN单元输入，然后再计算下一个时间步长上RNN的隐藏状态，以此重复...直到处理完输入文本中的每一个单词，由于输入文本的长度为n，所以要经历n个时间步长。\n",
    "\n",
    "如下是引入双向LSTM进行分类；一般流程是1. embeddding layer, 2.Bi-LSTM layer, 3.concat output, 4.FC layer, 5.softmax\n",
    "![cnn](12.png)\n",
    "\n",
    "   一般取前向/反向LSTM在最后一个时间步长上隐藏状态，然后进行拼接，在经过一个softmax层(输出层使用softmax激活函数)进行一个多分类；或者取前向/反向LSTM在每一个时间步长上的隐藏状态，对每一个时间步长上的两个隐藏状态进行拼接，然后对所有时间步长上拼接后的隐藏状态取均值，再经过一个softmax层(输出层使用softmax激活函数)进行一个多分类(2分类的话使用sigmoid激活函数)。\n",
    " \n",
    " \n",
    "## Recurrent Convolutional Neural Networks（RCNN）\n",
    " TextRNN可以获取上下文信息，但是单向RNN是有偏的模型(biased model)，后面的词占得重要性更大。TextCNN是无偏的模型(unbiased model)，能够通过最大池化获得最重要的特征，但是特征提取器大小固定，设定小了容易造成信息丢失，设定大了造成巨大的参数空间。\n",
    "\n",
    "为了解决TextRNN和TextCNN的模型缺陷，循环卷积神经网络(Recurrent Convolutional Neural Networks，RCNN)提出了：\n",
    "\n",
    "双向循环结构：比传统的基于窗口的神经网络噪声要小，能够最大化地提取上下文信息；\n",
    "max-pooling池化层：自动决策哪个特征更有重要作用。\n",
    "![cnn](13.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Dropout, LSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class TextRNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input = Input((self.maxlen,))\n",
    "\n",
    "        embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "        x = LSTM(128)(embedding)  # LSTM or GRU\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=input, outputs=output)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)...\n",
      "x_train shape: (25000, 400)\n",
      "x_test shape: (25000, 400)\n",
      "Build model...\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Train...\n",
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      " 2176/25000 [=>............................] - ETA: 14:04 - loss: 0.6847 - acc: 0.5648"
     ]
    }
   ],
   "source": [
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = TextRNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict(x_test)\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('Test data accuracy is ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/xinchenh/NLPLearning/blob/master/task08.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras import backend as K\n",
    "from keras.layers import Embedding, Dense, SimpleRNN, Lambda, Concatenate, Conv1D, GlobalMaxPooling1D, LSTM\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "class RCNN(object):\n",
    "    def __init__(self, maxlen, max_features, embedding_dims,\n",
    "                 class_num=1,\n",
    "                 last_activation='sigmoid'):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_features = max_features\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.class_num = class_num\n",
    "        self.last_activation = last_activation\n",
    "\n",
    "    def get_model(self):\n",
    "        input_current = Input((self.maxlen,))\n",
    "        input_left = Input((self.maxlen,))\n",
    "        input_right = Input((self.maxlen,))\n",
    "\n",
    "        embedder = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)\n",
    "        embedding_current = embedder(input_current)\n",
    "        embedding_left = embedder(input_left)\n",
    "        embedding_right = embedder(input_right)\n",
    "\n",
    "        x_left = LSTM(128, return_sequences=True)(embedding_left)\n",
    "        x_right = LSTM(128, return_sequences=True, go_backwards=True)(embedding_right)\n",
    "        x_right = Lambda(lambda x: K.reverse(x, axes=1))(x_right)\n",
    "        x = Concatenate(axis=2)([x_left, embedding_current, x_right])\n",
    "\n",
    "        x = Conv1D(64, kernel_size=1, activation='tanh')(x)\n",
    "        x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "        output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "        model = Model(inputs=[input_current, input_left, input_right], outputs=output)\n",
    "        return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "epochs = 10\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)...')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Prepare input for model...')\n",
    "x_train_current = x_train\n",
    "x_train_left = np.hstack([np.expand_dims(x_train[:, 0], axis=1), x_train[:, 0:-1]])\n",
    "x_train_right = np.hstack([x_train[:, 1:], np.expand_dims(x_train[:, -1], axis=1)])\n",
    "x_test_current = x_test\n",
    "x_test_left = np.hstack([np.expand_dims(x_test[:, 0], axis=1), x_test[:, 0:-1]])\n",
    "x_test_right = np.hstack([x_test[:, 1:], np.expand_dims(x_test[:, -1], axis=1)])\n",
    "print('x_train_current shape:', x_train_current.shape)\n",
    "print('x_train_left shape:', x_train_left.shape)\n",
    "print('x_train_right shape:', x_train_right.shape)\n",
    "print('x_test_current shape:', x_test_current.shape)\n",
    "print('x_test_left shape:', x_test_left.shape)\n",
    "print('x_test_right shape:', x_test_right.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = RCNN(maxlen, max_features, embedding_dims).get_model()\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit([x_train_current, x_train_left, x_train_right], y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=([x_test_current, x_test_left, x_test_right], y_test))\n",
    "\n",
    "print('Test...')\n",
    "result = model.predict([x_test_current, x_test_left, x_test_right])\n",
    "result[result>=0.5] = 1\n",
    "result[result<0.5] = 0\n",
    "acc = accuracy_score(result, y_test)\n",
    "print('Test data accuracy is ', acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
