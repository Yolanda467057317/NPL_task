{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF（term frequency–inverse document frequency）\n",
    "\n",
    "TF-IDF是一种用于信息检索（information retrieval）与文本挖掘（text mining）的常用加权技术。\n",
    "\n",
    "TF（词频），如果某个词比较少见，但是它在这篇文章中多次出现，那么它很可能就反映了这篇文章的特性。\n",
    "\n",
    "用统计语言来表示，是在词频的基础上,对每个词分配一个表示“重要性”的 权重 ，比如最常见的词（如停用词）给与最小的权重，而少见的词语给予较大的权重，这个权重叫做 “逆文档频率（IDF）”，它的大小与一个词的常见程度成反比。\n",
    "\n",
    "频率（TF） 和 逆文档频率（IDF） 相乘，就得到了一个词的 TF-IDF值 ，值越大，说明词对文章的重要性越高。\n",
    "    \n",
    "TF-IDF算法的优点是简单快速，结果比较符合实际情况。缺点是，单纯以\"词频\"衡量一个词的重要性，不够全面，有时重要的词可能出现次数并不多。而且，这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。\n",
    "\n",
    "（一种解决方法是，对全文的第一段和每一段的第一句话，给予较大的权重。）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 28)\t0.2323987345417178\n",
      "  (0, 26)\t0.2323987345417178\n",
      "  (0, 25)\t0.3664516633475799\n",
      "  (0, 15)\t0.4647974690834356\n",
      "  (0, 13)\t0.1483371018604668\n",
      "  (0, 10)\t0.4647974690834356\n",
      "  (0, 9)\t0.2323987345417178\n",
      "  (0, 8)\t0.2323987345417178\n",
      "  (0, 7)\t0.2323987345417178\n",
      "  (0, 6)\t0.18322583167378995\n",
      "  (0, 4)\t0.2323987345417178\n",
      "  (0, 3)\t0.2323987345417178\n",
      "  (1, 27)\t0.361536687086221\n",
      "  (1, 24)\t0.361536687086221\n",
      "  (1, 19)\t0.361536687086221\n",
      "  (1, 16)\t0.361536687086221\n",
      "  (1, 13)\t0.23076418416976147\n",
      "  (1, 12)\t0.28503967675464414\n",
      "  (1, 11)\t0.361536687086221\n",
      "  (1, 6)\t0.28503967675464414\n",
      "  (1, 5)\t0.361536687086221\n",
      "  (2, 22)\t0.5\n",
      "  (2, 14)\t0.5\n",
      "  (2, 2)\t0.5\n",
      "  (2, 1)\t0.5\n",
      "  (3, 30)\t0.3219014546209422\n",
      "  (3, 29)\t0.3219014546209422\n",
      "  (3, 25)\t0.25379080422375233\n",
      "  (3, 23)\t0.3219014546209422\n",
      "  (3, 21)\t0.3219014546209422\n",
      "  (3, 20)\t0.3219014546209422\n",
      "  (3, 18)\t0.3219014546209422\n",
      "  (3, 17)\t0.3219014546209422\n",
      "  (3, 13)\t0.20546552870565465\n",
      "  (3, 12)\t0.25379080422375233\n",
      "  (3, 0)\t0.3219014546209422\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from collections import Counter\n",
    "import jieba\n",
    "import pandas as pd\n",
    "\n",
    "line = [\"I come to China to travel with my gril friend but my gril freind is a boy\", \n",
    "    \"This is a car polupar in China which named hongqi\",          \n",
    "    \"I love tea and Apple \",   \n",
    "    \"The work is to write some papers in science or acta\"]\n",
    "\n",
    "vectorizer=CountVectorizer()\n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(vectorizer.fit_transform(line))  \n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acta', 'and', 'apple', 'boy', 'but', 'car', 'china', 'come', 'freind', 'friend', 'gril', 'hongqi', 'in', 'is', 'love', 'my', 'named', 'or', 'papers', 'polupar', 'science', 'some', 'tea', 'the', 'this', 'to', 'travel', 'which', 'with', 'work', 'write']\n",
      "{'come': 7, 'to': 25, 'china': 6, 'travel': 26, 'with': 28, 'my': 15, 'gril': 10, 'friend': 9, 'but': 4, 'freind': 8, 'is': 13, 'boy': 3, 'this': 24, 'car': 5, 'polupar': 19, 'in': 12, 'which': 27, 'named': 16, 'hongqi': 11, 'love': 14, 'tea': 22, 'and': 1, 'apple': 2, 'the': 23, 'work': 29, 'write': 30, 'some': 21, 'papers': 18, 'science': 20, 'or': 17, 'acta': 0}\n",
      "[[0.         0.         0.         0.23239873 0.23239873 0.\n",
      "  0.18322583 0.23239873 0.23239873 0.23239873 0.46479747 0.\n",
      "  0.         0.1483371  0.         0.46479747 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.36645166 0.23239873 0.         0.23239873 0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.36153669\n",
      "  0.28503968 0.         0.         0.         0.         0.36153669\n",
      "  0.28503968 0.23076418 0.         0.         0.36153669 0.\n",
      "  0.         0.36153669 0.         0.         0.         0.\n",
      "  0.36153669 0.         0.         0.36153669 0.         0.\n",
      "  0.        ]\n",
      " [0.         0.5        0.5        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.5        0.         0.         0.\n",
      "  0.         0.         0.         0.         0.5        0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.32190145 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.2537908  0.20546553 0.         0.         0.         0.32190145\n",
      "  0.32190145 0.         0.32190145 0.32190145 0.         0.32190145\n",
      "  0.         0.2537908  0.         0.         0.         0.32190145\n",
      "  0.32190145]]\n",
      "文本数 4\n",
      "单词数 31\n",
      "  (0, 7)\t0.2323987345417178\n",
      "  (0, 25)\t0.3664516633475799\n",
      "  (0, 6)\t0.18322583167378995\n",
      "  (0, 26)\t0.2323987345417178\n",
      "  (0, 28)\t0.2323987345417178\n",
      "  (0, 15)\t0.4647974690834356\n",
      "  (0, 10)\t0.4647974690834356\n",
      "  (0, 9)\t0.2323987345417178\n",
      "  (0, 4)\t0.2323987345417178\n",
      "  (0, 8)\t0.2323987345417178\n",
      "  (0, 13)\t0.1483371018604668\n",
      "  (0, 3)\t0.2323987345417178\n",
      "  (1, 6)\t0.28503967675464414\n",
      "  (1, 13)\t0.23076418416976147\n",
      "  (1, 24)\t0.361536687086221\n",
      "  (1, 5)\t0.361536687086221\n",
      "  (1, 19)\t0.361536687086221\n",
      "  (1, 12)\t0.28503967675464414\n",
      "  (1, 27)\t0.361536687086221\n",
      "  (1, 16)\t0.361536687086221\n",
      "  (1, 11)\t0.361536687086221\n",
      "  (2, 14)\t0.5\n",
      "  (2, 22)\t0.5\n",
      "  (2, 1)\t0.5\n",
      "  (2, 2)\t0.5\n",
      "  (3, 25)\t0.2537908042237523\n",
      "  (3, 13)\t0.20546552870565463\n",
      "  (3, 12)\t0.2537908042237523\n",
      "  (3, 23)\t0.32190145462094216\n",
      "  (3, 29)\t0.32190145462094216\n",
      "  (3, 30)\t0.32190145462094216\n",
      "  (3, 21)\t0.32190145462094216\n",
      "  (3, 18)\t0.32190145462094216\n",
      "  (3, 20)\t0.32190145462094216\n",
      "  (3, 17)\t0.32190145462094216\n",
      "  (3, 0)\t0.32190145462094216\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf2 = TfidfVectorizer()\n",
    "re = tfidf2.fit_transform(line)\n",
    "# 得到语料库all不重复的词\n",
    "print(tfidf2.get_feature_names())\n",
    "#得到每个词的对应 第几句和ID\n",
    "print(tfidf2.vocabulary_)\n",
    "#得到每个句子所对应的向量，向量里的数字的顺序是按词顺序来排列的\n",
    "# 文本矩阵化\n",
    "print(re.toarray())\n",
    "\n",
    "print(\"文本数\", len(re.toarray()))\n",
    "print(\"单词数\", len(re.toarray()[0]))\n",
    "print( re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "# from sklearn.feature_extraction.text import CountVectorizer  \n",
    "\n",
    "# corpus = [\"I love your eyes\", \"like the beautiful moon\"]\n",
    "\n",
    "# vectorizer = CountVectorizer()\n",
    "# transformer = TfidfTransformer()\n",
    "# tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))\n",
    "\n",
    "# print(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "互信息是信息论里的一种信息度量，它可以看成是一个随机变量中包含的关于另一个随机变量的信息量，或者说是一个随机变量由于已知另一个随机变量二减少的不确定性。 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个单词和后三个单词之间的互信息：\n",
      "0.3528681999138815\n",
      "0.0691103344933365\n",
      "0.3946507687941262\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn import metrics as mr\n",
    "\n",
    "m = re.toarray()\n",
    "\n",
    "print(\"第一个单词和后三个单词之间的互信息：\")\n",
    "print(mr.mutual_info_score(m[0], m[1]))\n",
    "print(mr.mutual_info_score(m[0], m[2]))\n",
    "print(mr.mutual_info_score(m[0], m[3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
