{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从one-hot到word2vec\n",
    "\n",
    "参考资料：\n",
    "1.one-hot向量与word2vec\n",
    "https://blog.csdn.net/mawenqi0729/article/details/80698780\n",
    "2.word2vec 中的数学原理详解（一）目录和前言 \n",
    "https://blog.csdn.net/itplus/article/details/37969519\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## one-hot向量\n",
    "\n",
    "词向量是通过一个数字组成的向量来表示一个词。假设在一个语料集合中，一共有n个不同的词，则可用一个长度为n的向量表示，例如只有第i个词存在，则向量index=i处值为1外，向量其他位置的值都为0，这样就可以唯一地通过一个[0,0,1,...,0,0]形式的向量表示一个词。\n",
    "\n",
    "one-hot向量比较简单也容易理解，但是有很多问题。比如当加入新词时，整个向量的长度会改变，并且存在维度过高难以计算的问题，以及向量的表示方法很难体现两个词之间的关系。\n",
    "\n",
    "优点：简单易懂、稀疏存储\n",
    "缺点：维度灾难、词汇鸿沟\n",
    "\n",
    "eg:\n",
    "已知三个feature，三个feature分别取值如下： feature1=[“male”, “female”] feature2=[“from Europe”, “from US”, “from Asia”] feature3=[“uses Firefox”, “uses Chrome”, “uses Safari”, “uses Internet Explorer”]\n",
    "\n",
    "如果做普通数据处理，那么我们就按0,1,2,3进行编号就行了。例如feature1=[0，1],feature2=[0，1，2],feature3=[0，1，2，3]。 \n",
    "\n",
    "如果某个样本为[“male”,“from Asia”, “uses Chrome”]，它就可以表示为[0，2，1]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4]\n",
      "[0 2 5 9]\n",
      "[[1. 0. 0. 1. 0. 0. 0. 0. 1.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function n_values_ is deprecated; The ``n_values_`` attribute was deprecated in version 0.20 and will be removed 0.22.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:77: DeprecationWarning: Function feature_indices_ is deprecated; The ``feature_indices_`` attribute was deprecated in version 0.20 and will be removed 0.22.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# sklearn实现one-hot encode\n",
    "from sklearn import preprocessing\n",
    "\n",
    "enc = preprocessing.OneHotEncoder()  # 创建对象\n",
    "enc.fit([[0,0,3],[1,1,0],[0,2,1],[1,0,2]])   # 拟合\n",
    "print(enc.n_values_) #每个特征有几种取值\n",
    "print(enc.feature_indices_) #特征索引\n",
    "array = enc.transform([[0,1,3]]).toarray()  # 转化\n",
    "print(array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]]) \n",
    "\n",
    "fit中，所有的数组第一个元素取值分别为：0，1，0，1，最大为1，且为两种元素（0，1），说明用2个状态位来表示就可以了，且该维度的value值为2\n",
    "\n",
    "所有的数组第二个元素取值分别为：0，1，2，0，最大为2，且为两种元素（0，1，2），说明用3个状态位来表示就可以了，且该维度的value值为3\n",
    "\n",
    "所有的数组第三个元素取值分别为：3，0，1，2，最大为3，且为两种元素（0，1，2，3），说明用4个状态位来表示就可以了，且该维度的value值为4\n",
    "\n",
    "所以整个的value值为（2，3，4），这也就解释了 enc.n_values_等于array([2,3,4])的原因。\n",
    "\n",
    "而enc.feature_indices_则是特征索引，该例子中value值为（2，3，4），则特征索引从0开始，到2的位置为第一个，到2+3=5的位置为第二个，到2+3+4的位置为第三个，索引为array([0,2,5,9])\n",
    "\n",
    "enc.transform([[0,1,3]]).toarray()\n",
    "第一个特征(可为0、1)在该样本中取0，编码为[1,0]\n",
    "第二个特征(可为0、1、2)在该样本中取1，编码为[0，1，0]\n",
    "第三个特征(可为0、1、2，3)在该样本中取3，编码为[0，0，0，1]\n",
    "连在一起就是[[1. 0. 0. 1. 0. 0. 0. 0. 1.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "2013年，Google开源了一款直接计算低微词向量的工具----Word2Vec，不仅能够在百万级的词典亿级数据集上高效训练，而且能够很好的度量词与词之间的相似性。\n",
    "\n",
    "对原始NNLM的改进：\n",
    "移除前向反馈神经网络中的非线性hidden layer，直接将中间层的embedding layer 与 softmax layer连接\n",
    "输入所有词向量到一个embedding layer 中\n",
    "将特征词嵌入上下文环境\n",
    "后续还在训练方法上进行了优化：层次softmax以及负采样技术\n",
    "\n",
    "word2vec两种训练方式：\n",
    "Continuous Bag of Words Model和Skip-Gram\n",
    "word2vec两种优化方式：\n",
    "hierarchical softmax 和negative sampling\n",
    "\n",
    "模型步骤见资料https://blog.csdn.net/mawenqi0729/article/details/80698780"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=None, # 被训练的文本数据\n",
    "                 corpus_file=None, # 语料的路径，可以替换掉sentence参数\n",
    "                 size=100, # 词向量的维度\n",
    "                 alpha=0.025, # 初始学习率\n",
    "                 window=5, # 滑动窗口，左右各5个词\n",
    "                 min_count=5, # 最小计数，如果单词统计数不够就不会进行训练\n",
    "                 max_vocab_size=None, # 构建词汇表时的内存限制，默认不限制\n",
    "                 sample=1e-3, # 对高频词进行下采样，采样比例阈值设置\n",
    "                 seed=1, # 随机种子，初始化词向量时使用，单词的哈希值+seed\n",
    "                 workers=3, # 线程数\n",
    "                 min_alpha=0.0001, # 学习率会随迭代次数线性衰减\n",
    "                 sg=0, # 1表示是skip-gram，0表示CBOW\n",
    "                 hs=0, # 1表示层级softmax，0表示负采样\n",
    "                 negative=5, # 如果>0表示使用负采样，官方实验推荐5-20\n",
    "                 ns_exponent=0.75, # 负采样指数分布，官方推荐0.75\n",
    "                 cbow_mean=1, # CBOW算法词向量的的合并，0表示使用词向量的和，1表示使用词向量的均值\n",
    "                 hashfxn=hash, # 哈希的方式初始化权重，有利于复现权重\n",
    "                 iter=5, # 迭代次数\n",
    "                 null_word=0, \n",
    "                 trim_rule=None, \n",
    "                 sorted_vocab=1, # 1表示按照单词出现频率的降序进行排序\n",
    "                 batch_words=MAX_WORDS_IN_BATCH, # 批次\n",
    "                 compute_loss=False, # 是否计算并保持损失\n",
    "                 callbacks=(),\n",
    "                 max_final_vocab=None)\n",
    "                 # 保存模型\n",
    "model.sava(fname)\n",
    "# 保存词向量，以数值的方式保存，非二进制\n",
    "model.wv.save_word2vec_format('../data/embedding.txt',binary=False)\n",
    "# 加载\n",
    "model = Word2Vec.load(fname)\n",
    "# 模型的使用\n",
    "model.most_similar(positive = ['woman','king'],negative = ['man'])\n",
    "# 输出：[('queen',0.50882536),...]\n",
    "model.doesnt_match(\"brekfast cereal dinner lunch\".split())\n",
    "# output:'cereal'\n",
    "model.similarity('woman','man')\n",
    "# output:0.737\n",
    "model['computer']\t# raw numpy vector of a word\n",
    "# output:array([-0.00449447, -0.00310097, 0.02421786, ...],dtype = float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=5, size=100, alpha=0.025)\n",
      "-0.010127383392667244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences= [[\"cat\", \"say\", \"meow\"],[\"dog\", \"say\", \"woof\"]]\n",
    "\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    " \n",
    "# sentences=word2vec.Text8Corpus(r'D:\\\\file_name)#训练集的格式为英文文本或分好词的中文文本\n",
    "\n",
    "print(model)\n",
    "#计算词向量的相似度\n",
    "sim1 = model.similarity('cat', 'dog')\n",
    "print(sim1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 30)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences= [[\"cat\", \"say\", \"meow\"],[\"dog\", \"say\", \"woof\"]]\n",
    "model_1 = Word2Vec(min_count=1)\n",
    "model_1.build_vocab(sentences)  # prepare the model vocabulary\n",
    "model_1.train(sentences, total_examples=model.corpus_count, epochs=model.iter)  # train word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
